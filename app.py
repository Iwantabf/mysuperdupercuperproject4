
import streamlit as st
import tensorflow as tf
import numpy as np
import joblib
import cv2
import mediapipe as mp
import tempfile

# -------------------------------
# CONFIG
# -------------------------------
SEQUENCE_LENGTH = 60  
mp_hands = mp.solutions.hands

# -------------------------------
# UI
# -------------------------------
st.title("‚úã Hearing assistive technology system")
st.write("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏∑‡∏≠ ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")

# -------------------------------
# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ Label Encoder
# -------------------------------
@st.cache_resource
def load_model_and_labels():
    model = tf.keras.models.load_model("signlang_lstm.h5")
    encoder = joblib.load("label_encoder.pkl")
    labels = list(encoder.classes_)
    return model, labels

model, labels = load_model_and_labels()

# -------------------------------
# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
# -------------------------------
uploaded_file = st.file_uploader("üìÇ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠", type=["mp4", "avi", "mov"])

if uploaded_file is not None:
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
    st.video(uploaded_file)

    # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    tfile = tempfile.NamedTemporaryFile(delete=False) 
    tfile.write(uploaded_file.read())

    # ‡∏≠‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏î‡πâ‡∏ß‡∏¢ OpenCV
    cap = cv2.VideoCapture(tfile.name)
    sequence = []

    with mp_hands.Hands(
        max_num_hands=2,
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5
    ) as hands:

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mediapipe
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = hands.process(image)

            landmarks_all = []
            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    for lm in hand_landmarks.landmark:
                        landmarks_all.extend([lm.x, lm.y, lm.z])

            # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤‡∏°‡∏µ 21 ‡∏à‡∏∏‡∏î‡∏Ñ‡∏£‡∏ö (2 ‡∏°‡∏∑‡∏≠ = 126 ‡∏Ñ‡πà‡∏≤)
            if len(landmarks_all) == 126:
                sequence.append(landmarks_all)

        cap.release()


    # -------------------------------
    # ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)
    # -------------------------------
    if len(sequence) > 0:
        seq_array = np.array(sequence)

        # ‡∏ñ‡πâ‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 60 ‚Üí pad ‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå
        if len(seq_array) < SEQUENCE_LENGTH:
            pad_length = SEQUENCE_LENGTH - len(seq_array)
            pad_array = np.zeros((pad_length, 126))  # 126 = keypoints ‡∏ï‡πà‡∏≠‡πÄ‡∏ü‡∏£‡∏°
            seq_array = np.vstack([seq_array, pad_array])

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 60 ‚Üí ‡πÄ‡∏≠‡∏≤ 60 ‡πÄ‡∏ü‡∏£‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        else:
            seq_array = seq_array[-SEQUENCE_LENGTH:]

        # reshape ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        seq_input = np.expand_dims(seq_array, axis=0)

        prediction = model.predict(seq_input, verbose=0)[0]
        max_index = np.argmax(prediction)
        predicted_label = labels[max_index]
        confidence = prediction[max_index]

        st.success(f"‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: **{predicted_label}**")
        st.write(f"üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {confidence:.2f}")
    else:
        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ keypoints ‡∏à‡∏≤‡∏Å‡∏°‡∏∑‡∏≠‡πÉ‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠")
